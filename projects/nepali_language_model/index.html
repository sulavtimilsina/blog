<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=content-language content="en"><meta name=color-scheme content="light dark"><meta name=author content="Sulav Timilsina"><meta name=description content="Nepali Bert"><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Nepali Language Modeling using RoBERTa"><meta name=twitter:description content="Nepali Bert"><meta property="og:title" content="Nepali Language Modeling using RoBERTa"><meta property="og:description" content="Nepali Bert"><meta property="og:type" content="article"><meta property="og:url" content="http://sulavtimilsina.github.io/projects/nepali_language_model/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2022-03-24T00:00:00+00:00"><meta property="article:modified_time" content="2022-03-24T00:00:00+00:00"><title>Nepali Language Modeling using RoBERTa Â· Sulav Timilsina</title><link rel=canonical href=http://sulavtimilsina.github.io/projects/nepali_language_model/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.d9fddbffe6f27e69985dc5fe0471cdb0e57fbf4775714bc3d847accb08f4a1f6.css integrity="sha256-2f3b/+byfmmYXcX+BHHNsOV/v0d1cUvD2Eesywj0ofY=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.002ee2378e14c7a68f1f0a53d9694ed252090987c4e768023fac694a4fc5f793.css integrity="sha256-AC7iN44Ux6aPHwpT2WlO0lIJCYfE52gCP6xpSk/F95M=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><meta name=generator content="Hugo 0.104.3"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Sulav Timilsina</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/publications/>Publications</a></li><li class=navigation-item><a class=navigation-link href=/posts>Blog</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/resume/resume.pdf>Resume</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class=navigation-item><a href=http://sulavtimilsina.github.io/ne/>ðŸ‡³ðŸ‡µ</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1 class=title><a class=title-link href=http://sulavtimilsina.github.io/projects/nepali_language_model/>Nepali Language Modeling using RoBERTa</a></h1></header><p>Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. It is at its core a transformer language model with a variable number of encoder layers and self-attention heads.
Here we used RoBERTa model wish is also based on BERT with 12 encoder layers. It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.</p><p>Since there were not any language models trained on Nepali language, I tried to train it on data collected from Nepali News Portals.
I scrapped about 14.5 GB of text data from more than 50 news sites. We trained two models, one with 128 token lengths and the other with 512 token lengths, using cloud TPUs. This is an active project, so code will be soon be made open source.</p><h2 id=find-more-about-this-project-here-link-to-project-sitehttpsnepbertagithubio>Find more about this project here. <a href=https://nepberta.github.io>Link to Project Site</a>
<a class=heading-link href=#find-more-about-this-project-here-link-to-project-sitehttpsnepbertagithubio><i class="fa fa-link" aria-hidden=true></i></a></h2><h3 id=other-contributers>Other Contributers:
<a class=heading-link href=#other-contributers><i class="fa fa-link" aria-hidden=true></i></a></h3><ol><li><a href=https://gautammilan.github.io>Milan Guatam</a></li></ol></article></section></div><footer class=footer><section class=container>Â©
2019 -
2022
Sulav Timilsina
Â·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.9cf2dbf9b6989ef8eae941ffb4231c26d1dc026bca38f1d19fdba50177d8a9ac.js integrity="sha256-nPLb+baYnvjq6UH/tCMcJtHcAmvKOPHRn9ulAXfYqaw="></script></body></html>