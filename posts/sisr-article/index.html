<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=content-language content="en"><meta name=color-scheme content="light dark"><meta name=author content="Sulav Timilsina"><meta name=description content="Introduction GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Single Image Super Resolution"><meta name=twitter:description content="Introduction GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution."><meta property="og:title" content="Single Image Super Resolution"><meta property="og:description" content="Introduction GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution."><meta property="og:type" content="article"><meta property="og:url" content="http://sulavtimilsina.github.io/posts/sisr-article/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-02T21:38:54+05:45"><meta property="article:modified_time" content="2022-02-02T21:38:54+05:45"><title>Single Image Super Resolution · Sulav Timilsina</title><link rel=canonical href=http://sulavtimilsina.github.io/posts/sisr-article/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.d9fddbffe6f27e69985dc5fe0471cdb0e57fbf4775714bc3d847accb08f4a1f6.css integrity="sha256-2f3b/+byfmmYXcX+BHHNsOV/v0d1cUvD2Eesywj0ofY=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.002ee2378e14c7a68f1f0a53d9694ed252090987c4e768023fac694a4fc5f793.css integrity="sha256-AC7iN44Ux6aPHwpT2WlO0lIJCYfE52gCP6xpSk/F95M=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><meta name=generator content="Hugo 0.107.0"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Sulav Timilsina</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/publications/>Publications</a></li><li class=navigation-item><a class=navigation-link href=/posts>Blog</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/resume/resume.pdf>Resume</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class=navigation-item><a href=http://sulavtimilsina.github.io/ne/>🇳🇵</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=http://sulavtimilsina.github.io/posts/sisr-article/>Single Image Super Resolution</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2022-02-02T21:38:54+05:45>February 2, 2022</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
8-minute read</span></div><div class=authors><i class="fa fa-user" aria-hidden=true></i>
<a href=/authors/sulav-timilsina/>Sulav Timilsina</a>
<span class=separator>•</span>
<a href=/authors/milan-gautam/>Milan Gautam</a></div><div class=categories><i class="fa fa-folder" aria-hidden=true></i>
<a href=/categories/deep-learning/>Deep Learning</a></div></div></header><div><h2 id=introduction>Introduction
<a class=heading-link href=#introduction><i class="fa fa-link" aria-hidden=true></i></a></h2><p><a href=https://arxiv.org/abs/1406.2661>GAN</a> (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution.</p><p>SISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information. Here the model&rsquo;s work is to map the function from low-resolution image data to its high-resolution image. Instead of giving a random noise to the Generator, a low-resolution image is fed into it. After passing through various Convolutional Layers and Upsampling Layers, the Generator gives a high-resolution image output. Generally, there are multiple solutions to this problem, so it&rsquo;s quite difficult to master the output up to original images in terms of richness and quality.</p><h2 id=data-preprocessing-and-augmentation>Data Preprocessing and Augmentation:
<a class=heading-link href=#data-preprocessing-and-augmentation><i class="fa fa-link" aria-hidden=true></i></a></h2><p>We have used the <a href=https://data.vision.ee.ethz.ch/cvl/DIV2K/>DIV2K</a> [Agustsson and Timofte (2017)] dataset provided by the TensorFlow library. There are altogether 800 pairs of low resolution and high-resolution images in the the training set whereas 100 pairs in the testing set. This data contains mainly people, cities, fauna, sceneries, etc.
We used flipping and rotating through 90, 180, and 270 degrees randomly over the dataset. Since we had limited memory on the training computer, we had to split large images into patches of smaller size. 19 patches of size 96 ✕ 96 pixels resolution were obtained from an image randomly. Our generator is designed to upsample images by 4 times so, the output image patch will be of dimension: 384 ✕ 384 pixels.</p><h2 id=generator>Generator
<a class=heading-link href=#generator><i class="fa fa-link" aria-hidden=true></i></a></h2><p>The generator is the block in the architecture which is responsible for generating the high resolution(HR) images from low resolution(LR) images. In 2015, <a href=https://arxiv.org/abs/1609.04802>SRGAN</a> was published which introduced the concept of using GAN for SISR tasks which produced the state the art solution. The generator of <a href=https://arxiv.org/abs/1609.04802>SRGAN</a> consists of several residual blocks that facilitate the flow of the gradient during backpropagation.</p><p><img src=/images/generator.png#center alt="Generator Architecture"></p><p>To further enhance the quality of generator images <a href=https://arxiv.org/abs/1809.00219>ESRGAN</a> was released which performed some modifications in the generator of the <a href=https://arxiv.org/abs/1609.04802>SRGAN</a> which includes:</p><ul><li>Removing the batch normalized(BN) layers.</li><li>Replacing the original residual block with the proposed Residual-in-Residual Dense Block (RRDB), which combines multi-level residual network and dense connections as in the figure below:</li></ul><p><img src=/images/rrdb.png#center alt="RRDB Diagram"></p><p>Fig: Residual in Residual Dense Block(RRDB)</p><p>Removing BN layers has proven to increase performance and reduce the computational complexity in different PSNR-oriented tasks including SR and deblurring. BN layers normalize the features using mean and variance in a batch during training and use the estimated mean and variance of the whole training dataset during testing. When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. The researchers empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework. These artifacts occasionally appear among iterations and different settings, violating the need for stable performance overtraining. Therefore, removing BN layers for stable training and consistent performance. Furthermore, removing BN layers helps to improve generalization ability and to reduce computational complexity and memory usage.</p><p>RRDB employs a deeper and more complex structure than the original residual block in SRGAN. Specifically, as shown in the figure above, the proposed RRDB has a residual-in-residual structure, where residual learning is used at different levels. Here, the RRDB uses dense block in the main path, where the network capacity becomes higher benefiting from the dense connections. In addition to the improved architecture, it also exploits several techniques to facilitate training a very deep network such as residual scaling(beta) i.e., scaling down the residuals by multiplying a constant between 0 and 1 before adding them to the main path to prevent instability.</p><h2 id=discriminator>Discriminator
<a class=heading-link href=#discriminator><i class="fa fa-link" aria-hidden=true></i></a></h2><p>The task of the discriminator is to discriminate between real HR images and generated SR images. Discriminator architecture here used is similar to DC-GAN architecture with LeakyReLU activation function. The network contains eight convolutional layers with 3×3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels as in the VGG network. Strided convolutions are used to reduce the image resolution each time the number of features is doubled.
But to overcome the instability while training of original GAN, we use a variant of GANs named improved training of Wasserstein GANs (WGAN-GP). So the last sigmoid layer of the conventional DC-GAN discriminator is omitted. This helps in not restricting the feature maps in 0 to 1 value.</p><p><img src=/images/discriminator.png#center alt="discriminator image"></p><h2 id=losses>Losses:
<a class=heading-link href=#losses><i class="fa fa-link" aria-hidden=true></i></a></h2><h3 id=generator-loss>Generator Loss
<a class=heading-link href=#generator-loss><i class="fa fa-link" aria-hidden=true></i></a></h3><p>The generator loss is the sum of MSE, perceptual loss +adversarial loss</p><p><em>lG = MSE+Perceptual Loss +Adversarial loss</em></p><p><em>lG= lMSE+lp+ lGA</em></p><h3 id=mean-square-errormse>Mean Square Error(MSE)
<a class=heading-link href=#mean-square-errormse><i class="fa fa-link" aria-hidden=true></i></a></h3><p>As the most common optimization objective for SISR, the pixelwise MSE loss is calculated as:</p><p><em>lMSE = ||GΘ(ILR) - IHR||22</em>,</p><p>where the parameter of the generator is denoted by ; the generated image, namely ISR,is denoted by GΘ(ILR); and the ground truth is denoted by IHR . Although models with MSE loss favor a high PSNR value, the generated results tend to be perceptually unsatisfying with overly smooth textures. Despite the aforementioned shortcomings, this loss term is still kept because MSE has clear physical meaning and helps to maintain color stability.</p><h1 id=mse-code-here>mse code here
<a class=heading-link href=#mse-code-here><i class="fa fa-link" aria-hidden=true></i></a></h1><h3 id=perceptual-loss>Perceptual Loss
<a class=heading-link href=#perceptual-loss><i class="fa fa-link" aria-hidden=true></i></a></h3><p>To compensate for the shortcomings of MSE loss and allow the loss function to better measure semantic and perceptual differences between images, we define and optimize a perceptual loss based on high-level features extracted from a pretrained network. The rationality of this loss term lies in that the pretrained network for classification originally has learned to encode the semantic and perceptual information that may be measured in the loss function. To enhance the performance of the perceptual loss, a 19-layer VGG network is used. The perceptual loss is actually the Euclidean distance between feature representations, which is defined as</p><p><em>lp = ||𝜙(GΘ(ILR)) - 𝜙(IHR)||22</em>,</p><p>where 𝜙 refers to the 19-layer VGG network. With this loss term, ISR and IHR are encouraged to have similar feature representations rather than to exactly match with each other in a pixel wise manner.</p><h1 id=vgg-code-here>vgg code here
<a class=heading-link href=#vgg-code-here><i class="fa fa-link" aria-hidden=true></i></a></h1><h2 id=adversarial-losses>Adversarial Losses:
<a class=heading-link href=#adversarial-losses><i class="fa fa-link" aria-hidden=true></i></a></h2><p>In <a href=https://arxiv.org/abs/1609.04802>SRGAN</a>, the adopted generative model is generative adversarial network (GAN) and it suffers from training instability. WGAN leverages the Wasserstein distance to produce a value function, which has better theoretical properties than the original GAN. However, WGAN requires that the discriminator must lie within the space of 1-Lipschitz through weight clipping, resulting in either vanishing or exploding gradients without careful tuning of the clipping threshold.<br>To overcome the flaw of clipping , a new approach is applied called Gradient Pelanty method. It is used to enforce the Lipschitz constraint. This way Wasserstein distance between two distributions to help decide when to stop the training but penalizes the gradient of the discriminator with respect to its input instead of weight clipping. With gradient penalty, the discriminator is encouraged to learn smoother decision boundaries.</p><h3 id=generator-loss-1>Generator Loss
<a class=heading-link href=#generator-loss-1><i class="fa fa-link" aria-hidden=true></i></a></h3><p><em>lGA=-𝔼[D(GΘ(ILR)]</em></p><h3 id=discriminator-loss>Discriminator Loss
<a class=heading-link href=#discriminator-loss><i class="fa fa-link" aria-hidden=true></i></a></h3><p><em>lDA=𝔼[D(GΘ(ILR)]-𝔼[D(IHR)] + λ𝔼(||▽hat{I}D(hat{I})-1||2-1)2</em></p><p><img src=/images/work_flow.png#center alt="workflow diagram"></p><p>Normally, the output of the classifier i.e. discriminator in this case is kept between 0-1 using a sigmoid function in the last layer, where if discriminator prediction 0 for an image then the image is SR likewise if the prediction is 1 then it is an HR image. Here the discriminator is trained using WGAN-GP approach <a href=https://sulavtimilsina.github.io/posts/wgan-gp/>(described here)</a>, hence the output is not bounded between 0-1 instead the discriminator will try to maximize the distance between the prediction of SR image and HR image and generator will try to minimize it. Let&rsquo;s look at the loss of the generator ie. IGA and the loss of discriminator IDA .</p><h3 id=understanding-discriminator-adversarial-loss>UNDERSTANDING DISCRIMINATOR ADVERSARIAL LOSS
<a class=heading-link href=#understanding-discriminator-adversarial-loss><i class="fa fa-link" aria-hidden=true></i></a></h3><p>(not considering the gradient penalty term for making it easier to understand)</p><p><em>lDA=𝔼[D(GΘ(ILR)]-𝔼[D(IHR)]</em></p><p>(Note: <em>lDA= 𝔼[D(IHR)]-𝔼[D(GΘ(ILR)]</em> if the loss of the discriminator is in this form than the discriminator will try to maximize this equation and the generator will try to minimize the IGA).</p><p>Considering D(GΘ(ILR))= 5 and D(IHR) = 5 initially when the discriminator doesn’t have the ability to differentiate between them.</p><p>Therefore the loss at the very beginning:
<em>lDA=5-5= 0,</em></p><p>The discriminator wants to minimize the loss lDA, hence increasing the distance between
D(GΘ(ILR))and D(IHR) . Suppose after the update of the gradient of the discriminator for the few step, the value of prediction becomes D(GΘ(ILR))=-2 and D(IHR) = 2 ,therefore discriminator is learning to know the difference between the LR image and the HR image, hence making the loss(lDA)= -4, Here the loss is minimized and the distance between the two predictions is maximized.</p><p><img src=/images/understanding_disc_adv_loss.png#center alt="discriminator adverserial loss"></p><h1 id=code>code
<a class=heading-link href=#code><i class="fa fa-link" aria-hidden=true></i></a></h1><h3 id=understanding-generator-adversarial-loss>UNDERSTANDING GENERATOR ADVERSARIAL LOSS
<a class=heading-link href=#understanding-generator-adversarial-loss><i class="fa fa-link" aria-hidden=true></i></a></h3><p>Discriminator is trained for a few steps and then the update of the generator happens. Therefore the discriminator is kept a few steps ahead of the generator in terms of its learning. Let&rsquo;s consider the discriminator has been trained for the few steps and it predicted outputs are:</p><p><em>D(GΘ(ILR)) = -2</em>
<em>D(IHR) = 2</em></p><p>The loss of the generator is:</p><p><em>lGA = -𝔼[D(GΘ(ILR)]</em></p><p>Therefore,
<em>lGA= -(-2) = 2</em></p><p>Generator wants to minimize lGA , which can only we achieved by increasing the value of D(GΘ(ILR)) hence ultimately reducing the distance between D(GΘ(ILR)) and D(IHR) ,hence making the SR image and HR image identical as:</p><p><em>lGA= -(large positive value) ≈ global minima</em></p><p><img src=/images/understanding_gen_adv_loss.png#center alt="generator adverserial loss"></p><h1 id=code-1>code
<a class=heading-link href=#code-1><i class="fa fa-link" aria-hidden=true></i></a></h1><h3 id=result-and-conclusion>Result and Conclusion:
<a class=heading-link href=#result-and-conclusion><i class="fa fa-link" aria-hidden=true></i></a></h3><p>We chose Kaggle&rsquo;s kernel with Tesla P100 GPU to train the model. Since it has 20 million training parameters, training it for 500 epochs is a tedious job. Until now we have trained it only up to 100 epochs.
Following is the sample output of the 100th epoch.
The rightmost image is Low-Resolution Patch, the Middle one is the High-Resolution Patch and the Left most one is the Generated High-Resolution Image.</p><p><img src=/images/output.png#center alt=outputs></p></div><footer><section class=see-also></section><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//sulav-blog.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article></section></div><footer class=footer><section class=container>©
2019 -
2022
Sulav Timilsina
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.9cf2dbf9b6989ef8eae941ffb4231c26d1dc026bca38f1d19fdba50177d8a9ac.js integrity="sha256-nPLb+baYnvjq6UH/tCMcJtHcAmvKOPHRn9ulAXfYqaw="></script></body></html>